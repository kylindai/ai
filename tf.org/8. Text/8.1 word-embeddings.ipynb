{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing text as numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first thing you must do is come up with a strategy\n",
    "# to conver strings to numbers(or to 'vectorize' the text)\n",
    "# there are three strategies for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this approach is inefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode each word with a unique number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two downsides to this approach\n",
    "# 1. the integer-encoding is arbitrary\n",
    "# 2. because there is no relationship between the similarity of any \n",
    "#    two words and the similarity of their encoding, this feature-weight\n",
    "#    combination is not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_unsup.txt',\n",
       " 'neg',\n",
       " 'urls_pos.txt',\n",
       " 'urls_neg.txt',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'labeledBow.feat']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = '../../data/aclImdb'\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "os.listdir(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    train_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b\"Wow. Some movies just leave me speechless. This was undeniably one of those movies. When I left the theatre, not a single word came to my mouth. All I had was an incredible urge to slam my head against the theatre wall to help me forget about the last hour and a half. Unfortunately, it didn't work. Honestly, this movie has nothing to recommend. The humor was at the first grade level, at best, the acting was overly silly, and the plot was astronomically far-fetched. I hearby pledge never to see an other movie starring Chris Kattan or any other cast-member of SNL.\"\n",
      "1 b'If any show in the last ten years deserves a 10, it is this rare gem. It allows us to escape back to a time when things were simpler and more fun. Filled with heart and laughs, this show keeps you laughing through the three decades of difference. The furniture was ugly, the clothes were colorful, and the even the drugs were tolerable. The hair was feathered, the music was accompanied by roller-skates, and in the words of Merle Haggard, \"a joint was a bad place to be\". Take a trip back to the greatest time in American history. Fall in love with characters and the feel good essence of the small town where people were nicer to each other. This classic is on television as much as \"Full House\". Don\\'t miss it, and always remember to \"Shake your groove thing!!!\"'\n",
      "1 b'Clearly an hilarious movie.<br /><br />It angers me to see the poor ratings given to this piece of comic genius<br /><br />Please look at this for what it is, a funny, ridiculous enjoyable film. Laugh for christ sake!<br /><br />'\n",
      "0 b\"Distasteful, cliched thriller has young couple doing cross-country research on America's most infamous murder sites, becoming road partners with a dim-witted young woman and her snarling boyfriend--who is an actual psycho. Arty and alienating, the film's tone alternates between pouty pseudo-irony and silly flamboyance. Handsomely-made perhaps, but ultimately laughable. Brad Pitt's performance as the low-rent killer is godawful. * from ****\"\n",
      "1 b\"Scott is right. The best 2 person sword duel ever put on film is in the middle of this movie. The sword fights with multiple fighters are not the best although quite good. However, the fight in the middle is the best even compared to Japanese samurai movies. Chinese swordplay scenes in my opinion have never surpassed the Japanese in terms of entertainment value. Especially in scenes where one guy must battle a group of enemies, Japanese movies excel, example being the Lone Wolf and Cub series. Even though duels in Japanese cinema last only seconds or a minute at the most, the sheer intensity of those moments made them better. But, this is one example where Chinese swordplay surpasses the Japanese. The scene in the middle of this film was a five minute long fight with the most amazing choreography ever. The other fights in this movie are good too but even if they sucked this movie would get a 7 for that one scene. If you haven't seen it, you have to. John Woo is the man.\"\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(5):\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02982171,  0.0093401 ,  0.00635153, -0.01102989, -0.01340748],\n",
       "       [ 0.04729998, -0.01149049, -0.04771422,  0.02998041,  0.01231867],\n",
       "       [ 0.01844174,  0.02626921,  0.02673053,  0.02100826, -0.04627996],\n",
       "       [ 0.02982171,  0.0093401 ,  0.00635153, -0.01102989, -0.01340748]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([1,2,3,1]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 3, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0,1,2], [3,4,5]]))\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.02983781, -0.0420953 , -0.03969896, -0.04420767,\n",
       "         -0.02993977],\n",
       "        [ 0.02982171,  0.0093401 ,  0.00635153, -0.01102989,\n",
       "         -0.01340748],\n",
       "        [ 0.04729998, -0.01149049, -0.04771422,  0.02998041,\n",
       "          0.01231867]],\n",
       "\n",
       "       [[ 0.01844174,  0.02626921,  0.02673053,  0.02100826,\n",
       "         -0.04627996],\n",
       "        [-0.0037847 ,  0.02906689, -0.01886067,  0.0402845 ,\n",
       "          0.0196395 ],\n",
       "        [-0.02685142,  0.02357025,  0.04514344,  0.01985129,\n",
       "          0.01879085]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase,\n",
    "                                            '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                   '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim, name='embedding'),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.6932 - accuracy: 0.4922WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "20/20 [==============================] - 3s 144ms/step - loss: 0.6924 - accuracy: 0.5028 - val_loss: 0.6914 - val_accuracy: 0.4886\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.6898 - accuracy: 0.5028 - val_loss: 0.6882 - val_accuracy: 0.4886\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 0.6852 - accuracy: 0.5028 - val_loss: 0.6825 - val_accuracy: 0.4886\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 0.6773 - accuracy: 0.5028 - val_loss: 0.6729 - val_accuracy: 0.4886\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 1s 71ms/step - loss: 0.6637 - accuracy: 0.5028 - val_loss: 0.6576 - val_accuracy: 0.4886\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 0.6444 - accuracy: 0.5028 - val_loss: 0.6369 - val_accuracy: 0.4886\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.6191 - accuracy: 0.5029 - val_loss: 0.6118 - val_accuracy: 0.4886\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.5893 - accuracy: 0.5330 - val_loss: 0.5839 - val_accuracy: 0.5768\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.5568 - accuracy: 0.6375 - val_loss: 0.5552 - val_accuracy: 0.6378\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.5238 - accuracy: 0.6982 - val_loss: 0.5276 - val_accuracy: 0.6780\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.4920 - accuracy: 0.7436 - val_loss: 0.5023 - val_accuracy: 0.7158\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.4625 - accuracy: 0.7724 - val_loss: 0.4800 - val_accuracy: 0.7398\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 2s 78ms/step - loss: 0.4358 - accuracy: 0.7958 - val_loss: 0.4608 - val_accuracy: 0.7538\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 2s 75ms/step - loss: 0.4119 - accuracy: 0.8124 - val_loss: 0.4447 - val_accuracy: 0.7632\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 1s 72ms/step - loss: 0.3906 - accuracy: 0.8272 - val_loss: 0.4312 - val_accuracy: 0.7770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd32129ee10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_1 (TextVe (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it']\n",
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vocab[:10])\n",
    "\n",
    "# get weights matrix of layer named 'embedding'\n",
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(vocab):\n",
    "    if num == 0: continue\n",
    "    vec = weights[num]\n",
    "    out_m.write(word + '\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vesc.tsv')\n",
    "    files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
