{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.functions import numerical_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的梯度：<p/>\n",
    "权重矩阵W：<p/>\n",
    "$ \n",
    "\\begin{split}\n",
    "W = \\begin{pmatrix} \n",
    "w_{11} & w_{12} & w_{13} \\\\ \n",
    "w_{21} & w_{22} & w_{23} \\\\ \n",
    "\\end{pmatrix} \n",
    "\\end{split}\n",
    "$ <p/>\n",
    "损失函数L关于权重参数的梯度：<p/>\n",
    "$\n",
    "\\begin{split}\n",
    "\\dfrac{\\delta L}{\\delta W} = \\begin{pmatrix}\n",
    "\\dfrac{\\delta L}{\\delta w_{11}} & \\dfrac{\\delta L}{\\delta w_{12}} & \\dfrac{\\delta L}{\\delta w_{13}} \\\\\n",
    "\\dfrac{\\delta L}{\\delta w_{21}} & \\dfrac{\\delta L}{\\delta w_{22}} & \\dfrac{\\delta L}{\\delta w_{23}} \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 符合正太分布的随机数矩阵\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        self.W = self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        return np.array([[0.47355232,0.9977393,0.84668094],\n",
    "                         [0.85557411,0.03563661,0.69422093]])\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # 输入与权重矩阵的点积运算\n",
    "        y = np.dot(x, self.W)\n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "#         print('enter - loss, x =', x, 't =', t)\n",
    "#         print('enter - loss, W =', self.W)\n",
    "        # 求预测值\n",
    "        z = self.predict(x)\n",
    "        print('enter - loss, predict =', z)\n",
    "        # 多分类\n",
    "        y = softmax(z)\n",
    "        print('enter - loss, softmax =', y)\n",
    "        # 交叉熵误差\n",
    "        l = cross_entropy_error(y, t)\n",
    "        print('enter - loss, cross_entropy_error =', l)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.47355232 0.9977393  0.84668094]\n",
      " [0.85557411 0.03563661 0.69422093]]\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print(net.W)\n",
    "# print(net.W.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05414809 0.63071653 1.1328074 ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y = np.argmax(p)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = np.zeros_like(p)\n",
    "t[y] = 1\n",
    "print(t)\n",
    "# print(net.loss(x, t))\n",
    "# print(x.size)\n",
    "# print(t.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lost_function -->\n",
      "enter - loss, predict = [1.05405809 0.63056653 1.1326574 ]\n",
      "enter - loss, softmax = [0.36542662 0.23926553 0.39530784]\n",
      "enter - loss, cross_entropy_error = 0.928090210960927\n",
      "lost_function --> 0.928090210960927 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05393809 0.63056653 1.1326574 ]\n",
      "enter - loss, softmax = [0.3653988  0.23927602 0.39532518]\n",
      "enter - loss, cross_entropy_error = 0.9280463614466788\n",
      "lost_function --> 0.9280463614466788 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05393809 0.63062653 1.1326574 ]\n",
      "enter - loss, softmax = [0.36539355 0.23928694 0.3953195 ]\n",
      "enter - loss, cross_entropy_error = 0.9280607183320589\n",
      "lost_function --> 0.9280607183320589 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05393809 0.63050653 1.1326574 ]\n",
      "enter - loss, softmax = [0.36540404 0.2392651  0.39533085]\n",
      "enter - loss, cross_entropy_error = 0.9280320052165812\n",
      "lost_function --> 0.9280320052165812 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05393809 0.63050653 1.1327174 ]\n",
      "enter - loss, softmax = [0.36539538 0.23925943 0.3953452 ]\n",
      "enter - loss, cross_entropy_error = 0.9279957255073139\n",
      "lost_function --> 0.9279957255073139 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05393809 0.63050653 1.1325974 ]\n",
      "enter - loss, softmax = [0.36541271 0.23927078 0.39531651]\n",
      "enter - loss, cross_entropy_error = 0.9280682857864075\n",
      "lost_function --> 0.9280682857864075 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05402809 0.63050653 1.1325974 ]\n",
      "enter - loss, softmax = [0.36543358 0.23926291 0.39530351]\n",
      "enter - loss, cross_entropy_error = 0.9281011738612368\n",
      "lost_function --> 0.9281011738612368 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05384809 0.63050653 1.1325974 ]\n",
      "enter - loss, softmax = [0.36539184 0.23927865 0.39532951]\n",
      "enter - loss, cross_entropy_error = 0.9280353995898565\n",
      "lost_function --> 0.9280353995898565 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05384809 0.63059653 1.1325974 ]\n",
      "enter - loss, softmax = [0.36538397 0.23929503 0.395321  ]\n",
      "enter - loss, cross_entropy_error = 0.9280569353997327\n",
      "lost_function --> 0.9280569353997327 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05384809 0.63041653 1.1325974 ]\n",
      "enter - loss, softmax = [0.36539971 0.23926226 0.39533803]\n",
      "enter - loss, cross_entropy_error = 0.9280138652543771\n",
      "lost_function --> 0.9280138652543771 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05384809 0.63041653 1.1326874 ]\n",
      "enter - loss, softmax = [0.36538671 0.23925375 0.39535954]\n",
      "enter - loss, cross_entropy_error = 0.9279594466586167\n",
      "lost_function --> 0.9279594466586167 \n",
      "\n",
      "lost_function -->\n",
      "enter - loss, predict = [1.05384809 0.63041653 1.1325074 ]\n",
      "enter - loss, softmax = [0.36541271 0.23927078 0.39531651]\n",
      "enter - loss, cross_entropy_error = 0.9280682857864075\n",
      "lost_function --> 0.9280682857864075 \n",
      "\n",
      "dW= [[ 0.21924757  0.14356558 -0.3628014 ]\n",
      " [ 0.32887136  0.21535073 -0.54419564]]\n"
     ]
    }
   ],
   "source": [
    "# 损失函数\n",
    "def loss_fucntion(W):\n",
    "    print('lost_function -->')\n",
    "    # SimpleNet.loss\n",
    "    l = net.loss(x, t)\n",
    "    print('lost_function -->', l, '\\n')\n",
    "    return l\n",
    "\n",
    "# 求损失函数的梯度\n",
    "dW = numerical_gradient(loss_fucntion, net.W)\n",
    "print('dW=', dW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[ 5 25]\n",
      "[10 20]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2],[3,4]])\n",
    "print(np.sum(a**2))\n",
    "print(np.sum(a**2, axis=1))\n",
    "print(np.sum(a**2, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 x= [1 2]\n",
      "i= 1 x= [3 4]\n"
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(a):\n",
    "    print('i=', i, 'x=', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    print(a[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
