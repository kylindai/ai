{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numerical gradient\n",
    "\n",
    "由全部的变量的偏导数汇总而成的向量成为梯度（gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导数：（函数值增量与自变量增量的比值的极限）<p/>\n",
    "$ \\dfrac{\\Delta{f(x)}}{\\Delta{x}} = \\lim_{h\\to0} \\dfrac{f(x+h)-f(x-h)}{2h} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度：（偏导数向量 - 偏导数只多个自变量中的其中一个的导数）<p/>\n",
    "$ \\Bigg(\\dfrac{\\Delta{f}}{\\Delta{x_0}},\\dfrac{\\Delta{f}}{\\Delta{x_1}}\\Bigg) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降：（沿梯度方向前进，逐渐减小函数值的过程，用来求损失函数最小值时的权重和偏置）<p/>\n",
    "$\n",
    "\\begin{align} \n",
    "x_0 = x_0 - \\eta\\dfrac{\\Delta{f}}{\\Delta{x_0}} \\\\\n",
    "x_1 = x_1 - \\eta\\dfrac{\\Delta{f}}{\\Delta{x_1}} \n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求导数 - 中心差分\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# 求梯度 - 全部变量的偏导数向量\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "    return grad\n",
    "\n",
    "# 求梯度下降 - 迭代（全部变量 - 学习率 * 梯度）\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        print(i, 'x=', x, 'after grad=', grad)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x_0 + x_1) = x_0^2 + x_1^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    return np.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 求函数foo的梯度\n",
    "print(numerical_gradient(foo, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(foo, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(foo, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x= [-2.4001  3.1999] after grad= [-6.  8.]\n",
      "1 x= [-1.92018  2.55982] after grad= [-4.8002  6.3998]\n",
      "2 x= [-1.536244  2.047756] after grad= [-3.84036  5.11964]\n",
      "3 x= [-1.2290952  1.6381048] after grad= [-3.072488  4.095512]\n",
      "4 x= [-0.98337616  1.31038384] after grad= [-2.4581904  3.2762096]\n",
      "5 x= [-0.78680093  1.04820707] after grad= [-1.96675232  2.62076768]\n",
      "6 x= [-0.62954074  0.83846566] after grad= [-1.57360186  2.09641414]\n",
      "7 x= [-0.50373259  0.67067253] after grad= [-1.25908148  1.67693132]\n",
      "8 x= [-0.40308608  0.53643802] after grad= [-1.00746519  1.34134505]\n",
      "9 x= [-0.32256886  0.42905042] after grad= [-0.80617215  1.07287604]\n",
      "10 x= [-0.25815509  0.34314033] after grad= [-0.64513772  0.85810083]\n",
      "11 x= [-0.20662407  0.27441227] after grad= [-0.51631018  0.68628067]\n",
      "12 x= [-0.16539926  0.21942981] after grad= [-0.41324814  0.54882453]\n",
      "13 x= [-0.13241941  0.17544385] after grad= [-0.33079851  0.43885963]\n",
      "14 x= [-0.10603552  0.14025508] after grad= [-0.26483881  0.3508877 ]\n",
      "15 x= [-0.08492842  0.11210406] after grad= [-0.21207105  0.28051016]\n",
      "16 x= [-0.06804274  0.08958325] after grad= [-0.16985684  0.22420813]\n",
      "17 x= [-0.05453419  0.0715666 ] after grad= [-0.13608547  0.1791665 ]\n",
      "18 x= [-0.04372735  0.05715328] after grad= [-0.10906838  0.1431332 ]\n",
      "19 x= [-0.03508188  0.04562262] after grad= [-0.0874547   0.11430656]\n",
      "[-0.03508188  0.04562262]\n",
      "0.0033121622344551805\n"
     ]
    }
   ],
   "source": [
    "# 求函数foo的最小值 - 即求函数 f(x1+x2) = x0**2 + x1**2，x0和x1的最小值\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "min_x = gradient_descent(foo, init_x=init_x, lr=0.1, step_num=20)\n",
    "print(min_x)\n",
    "print(foo(min_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
