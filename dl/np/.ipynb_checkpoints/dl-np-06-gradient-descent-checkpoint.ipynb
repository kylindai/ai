{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numerical gradient\n",
    "\n",
    "由全部的变量的偏导数汇总而成的向量成为梯度（gradient）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导数：（函数值增量与自变量增量的比值的极限）<p/>\n",
    "$ \\dfrac{\\Delta{f(x)}}{\\Delta{x}} = \\lim_{h\\to0} \\dfrac{f(x+h)-f(x-h)}{2h} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度：（偏导数向量 - 偏导数只多个自变量中的其中一个的导数）<p/>\n",
    "$ \\Bigg(\\dfrac{\\Delta{f}}{\\Delta{x_0}},\\dfrac{\\Delta{f}}{\\Delta{x_1}}\\Bigg) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降：（沿梯度方向前进，逐渐减小函数值的过程，用来求损失函数最小值时的权重和偏置）<p/>\n",
    "$\n",
    "\\begin{align} \n",
    "x_0 = x_0 - \\eta\\dfrac{\\Delta{f}}{\\Delta{x_0}} \\\\\n",
    "x_1 = x_1 - \\eta\\dfrac{\\Delta{f}}{\\Delta{x_1}} \n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求导数 - 中心差分\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# 求梯度 - 全部变量的偏导数向量\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        # f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "    return grad\n",
    "\n",
    "# 求梯度下降 - 迭代（全部变量 - 学习率 * 梯度）\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "        print(i, 'x=', x, 'grad=', grad)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ f(x_0 + x_1) = x_0^2 + x_1^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    return np.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 求函数的梯度\n",
    "print(numerical_gradient(foo, np.array([3.0, 4.0])))\n",
    "print(numerical_gradient(foo, np.array([0.0, 2.0])))\n",
    "print(numerical_gradient(foo, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 x= [-2.4001  3.1999] grad= [-6.  8.]\n",
      "1 x= [-1.92018  2.55982] grad= [-4.8002  6.3998]\n",
      "2 x= [-1.536244  2.047756] grad= [-3.84036  5.11964]\n",
      "3 x= [-1.2290952  1.6381048] grad= [-3.072488  4.095512]\n",
      "4 x= [-0.98337616  1.31038384] grad= [-2.4581904  3.2762096]\n",
      "5 x= [-0.78680093  1.04820707] grad= [-1.96675232  2.62076768]\n",
      "6 x= [-0.62954074  0.83846566] grad= [-1.57360186  2.09641414]\n",
      "7 x= [-0.50373259  0.67067253] grad= [-1.25908148  1.67693132]\n",
      "8 x= [-0.40308608  0.53643802] grad= [-1.00746519  1.34134505]\n",
      "9 x= [-0.32256886  0.42905042] grad= [-0.80617215  1.07287604]\n",
      "10 x= [-0.25815509  0.34314033] grad= [-0.64513772  0.85810083]\n",
      "11 x= [-0.20662407  0.27441227] grad= [-0.51631018  0.68628067]\n",
      "12 x= [-0.16539926  0.21942981] grad= [-0.41324814  0.54882453]\n",
      "13 x= [-0.13241941  0.17544385] grad= [-0.33079851  0.43885963]\n",
      "14 x= [-0.10603552  0.14025508] grad= [-0.26483881  0.3508877 ]\n",
      "15 x= [-0.08492842  0.11210406] grad= [-0.21207105  0.28051016]\n",
      "16 x= [-0.06804274  0.08958325] grad= [-0.16985684  0.22420813]\n",
      "17 x= [-0.05453419  0.0715666 ] grad= [-0.13608547  0.1791665 ]\n",
      "18 x= [-0.04372735  0.05715328] grad= [-0.10906838  0.1431332 ]\n",
      "19 x= [-0.03508188  0.04562262] grad= [-0.0874547   0.11430656]\n",
      "20 x= [-0.0281655  0.0363981] grad= [-0.07016376  0.09124525]\n",
      "21 x= [-0.0226324   0.02901848] grad= [-0.05633101  0.0727962 ]\n",
      "22 x= [-0.01820592  0.02311478] grad= [-0.04526481  0.05803696]\n",
      "23 x= [-0.01466474  0.01839183] grad= [-0.03641185  0.04622957]\n",
      "24 x= [-0.01183179  0.01461346] grad= [-0.02932948  0.03678365]\n",
      "25 x= [-0.00956543  0.01159077] grad= [-0.02366358  0.02922692]\n",
      "26 x= [-0.00775235  0.00917262] grad= [-0.01913086  0.02318154]\n",
      "27 x= [-0.00630188  0.00723809] grad= [-0.01550469  0.01834523]\n",
      "28 x= [-0.0051415   0.00569047] grad= [-0.01260375  0.01447618]\n",
      "29 x= [-0.0042132   0.00445238] grad= [-0.010283    0.01138095]\n",
      "30 x= [-0.00347056  0.0034619 ] grad= [-0.0084264   0.00890476]\n",
      "31 x= [-0.00287645  0.00266952] grad= [-0.00694112  0.00692381]\n",
      "32 x= [-0.00240116  0.00203562] grad= [-0.0057529   0.00533905]\n",
      "33 x= [-0.00202093  0.00152849] grad= [-0.00480232  0.00407124]\n",
      "34 x= [-0.00171674  0.0011228 ] grad= [-0.00404185  0.00305699]\n",
      "35 x= [-0.00147339  0.00079824] grad= [-0.00343348  0.00224559]\n",
      "36 x= [-0.00127871  0.00053859] grad= [-0.00294679  0.00159647]\n",
      "37 x= [-0.00112297  0.00033087] grad= [-0.00255743  0.00107718]\n",
      "38 x= [-0.00099838  0.0001647 ] grad= [-0.00224594  0.00066174]\n",
      "39 x= [-8.98701937e-04  3.17576597e-05] grad= [-0.00199675  0.00032939]\n",
      "40 x= [-8.18961550e-04 -7.45938722e-05] grad= [-1.79740387e-03  6.35153194e-05]\n",
      "41 x= [-0.00075517 -0.00015968] grad= [-0.00163792 -0.00014919]\n",
      "42 x= [-0.00070414 -0.00022774] grad= [-0.00151034 -0.00031935]\n",
      "43 x= [-0.00066331 -0.00028219] grad= [-0.00140827 -0.00045548]\n",
      "44 x= [-0.00063065 -0.00032575] grad= [-0.00132662 -0.00056438]\n",
      "45 x= [-0.00060452 -0.0003606 ] grad= [-0.00126129 -0.00065151]\n",
      "46 x= [-0.00058361 -0.00038848] grad= [-0.00120903 -0.00072121]\n",
      "47 x= [-0.00056689 -0.00041079] grad= [-0.00116723 -0.00077696]\n",
      "48 x= [-0.00055351 -0.00042863] grad= [-0.00113378 -0.00082157]\n",
      "49 x= [-0.00054281 -0.0004429 ] grad= [-0.00110703 -0.00085726]\n",
      "50 x= [-0.00053425 -0.00045432] grad= [-0.00108562 -0.00088581]\n",
      "51 x= [-0.0005274  -0.00046346] grad= [-0.0010685  -0.00090864]\n",
      "52 x= [-0.00052192 -0.00047077] grad= [-0.0010548  -0.00092692]\n",
      "53 x= [-0.00051754 -0.00047661] grad= [-0.00104384 -0.00094153]\n",
      "54 x= [-0.00051403 -0.00048129] grad= [-0.00103507 -0.00095323]\n",
      "55 x= [-0.00051122 -0.00048503] grad= [-0.00102806 -0.00096258]\n",
      "56 x= [-0.00050898 -0.00048803] grad= [-0.00102244 -0.00097006]\n",
      "57 x= [-0.00050718 -0.00049042] grad= [-0.00101796 -0.00097605]\n",
      "58 x= [-0.00050575 -0.00049234] grad= [-0.00101436 -0.00098084]\n",
      "59 x= [-0.0005046  -0.00049387] grad= [-0.00101149 -0.00098467]\n",
      "60 x= [-0.00050368 -0.0004951 ] grad= [-0.00100919 -0.00098774]\n",
      "61 x= [-0.00050294 -0.00049608] grad= [-0.00100735 -0.00099019]\n",
      "62 x= [-0.00050235 -0.00049686] grad= [-0.00100588 -0.00099215]\n",
      "63 x= [-0.00050188 -0.00049749] grad= [-0.00100471 -0.00099372]\n",
      "64 x= [-0.00050151 -0.00049799] grad= [-0.00100377 -0.00099498]\n",
      "65 x= [-0.00050121 -0.00049839] grad= [-0.00100301 -0.00099598]\n",
      "66 x= [-0.00050096 -0.00049871] grad= [-0.00100241 -0.00099679]\n",
      "67 x= [-0.00050077 -0.00049897] grad= [-0.00100193 -0.00099743]\n",
      "68 x= [-0.00050062 -0.00049918] grad= [-0.00100154 -0.00099794]\n",
      "69 x= [-0.00050049 -0.00049934] grad= [-0.00100123 -0.00099835]\n",
      "70 x= [-0.00050039 -0.00049947] grad= [-0.00100099 -0.00099868]\n",
      "71 x= [-0.00050032 -0.00049958] grad= [-0.00100079 -0.00099895]\n",
      "72 x= [-0.00050025 -0.00049966] grad= [-0.00100063 -0.00099916]\n",
      "73 x= [-0.0005002  -0.00049973] grad= [-0.00100051 -0.00099933]\n",
      "74 x= [-0.00050016 -0.00049978] grad= [-0.0010004  -0.00099946]\n",
      "75 x= [-0.00050013 -0.00049983] grad= [-0.00100032 -0.00099957]\n",
      "76 x= [-0.0005001  -0.00049986] grad= [-0.00100026 -0.00099965]\n",
      "77 x= [-0.00050008 -0.00049989] grad= [-0.00100021 -0.00099972]\n",
      "78 x= [-0.00050007 -0.00049991] grad= [-0.00100017 -0.00099978]\n",
      "79 x= [-0.00050005 -0.00049993] grad= [-0.00100013 -0.00099982]\n",
      "80 x= [-0.00050004 -0.00049994] grad= [-0.00100011 -0.00099986]\n",
      "81 x= [-0.00050003 -0.00049995] grad= [-0.00100008 -0.00099989]\n",
      "82 x= [-0.00050003 -0.00049996] grad= [-0.00100007 -0.00099991]\n",
      "83 x= [-0.00050002 -0.00049997] grad= [-0.00100005 -0.00099993]\n",
      "84 x= [-0.00050002 -0.00049998] grad= [-0.00100004 -0.00099994]\n",
      "85 x= [-0.00050001 -0.00049998] grad= [-0.00100003 -0.00099995]\n",
      "86 x= [-0.00050001 -0.00049999] grad= [-0.00100003 -0.00099996]\n",
      "87 x= [-0.00050001 -0.00049999] grad= [-0.00100002 -0.00099997]\n",
      "88 x= [-0.00050001 -0.00049999] grad= [-0.00100002 -0.00099998]\n",
      "89 x= [-0.00050001 -0.00049999] grad= [-0.00100001 -0.00099998]\n",
      "90 x= [-0.0005     -0.00049999] grad= [-0.00100001 -0.00099998]\n",
      "91 x= [-0.0005 -0.0005] grad= [-0.00100001 -0.00099999]\n",
      "92 x= [-0.0005 -0.0005] grad= [-0.00100001 -0.00099999]\n",
      "93 x= [-0.0005 -0.0005] grad= [-0.00100001 -0.00099999]\n",
      "94 x= [-0.0005 -0.0005] grad= [-0.001      -0.00099999]\n",
      "95 x= [-0.0005 -0.0005] grad= [-0.001 -0.001]\n",
      "96 x= [-0.0005 -0.0005] grad= [-0.001 -0.001]\n",
      "97 x= [-0.0005 -0.0005] grad= [-0.001 -0.001]\n",
      "98 x= [-0.0005 -0.0005] grad= [-0.001 -0.001]\n",
      "99 x= [-0.0005 -0.0005] grad= [-0.001 -0.001]\n",
      "[-0.0005 -0.0005]\n"
     ]
    }
   ],
   "source": [
    "# 求函数foo的最小值 - 即求函数 f(x1+x2) = x0**2 + x1**2，x0和x1的最小值\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "des = gradient_descent(foo, init_x=init_x, lr=0.1, step_num=100)\n",
    "print(des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
